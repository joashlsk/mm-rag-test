{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joashlsk/mm-rag-test/blob/main/main_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goipOFI01cpX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d76aa5f-ce67-41b3-e148-ce21546e65f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: huggingface-hub 1.4.0 does not provide the extra 'inference'\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h‚úÖ Libraries Installed.\n"
          ]
        }
      ],
      "source": [
        "import streamlit as st\n",
        "from pinecone import Pinecone\n",
        "from groq import Groq\n",
        "from fastembed import TextEmbedding\n",
        "import time\n",
        "\n",
        "# --- CONFIGURATION (Secrets) ---\n",
        "# In Streamlit Cloud, you will set these in the \"Secrets\" menu, not hardcode them.\n",
        "# For local testing, you can uncomment below (but don't commit to GitHub!):\n",
        "# os.environ[\"GROQ_API_KEY\"] = \"your-groq-key\"\n",
        "# os.environ[\"PINECONE_API_KEY\"] = \"your-pinecone-key\"\n",
        "\n",
        "# --- 1. SETUP UI ---\n",
        "st.set_page_config(page_title=\"Project Qwen RAG\", layout=\"wide\")\n",
        "st.title(\"ü§ñ Qwen 2.5-32B Enterprise RAG\")\n",
        "st.markdown(\"Running on **Groq** (LLM) + **Pinecone** (Vector DB) + **Streamlit** (UI)\")\n",
        "\n",
        "# --- 2. INITIALIZE CLIENTS ---\n",
        "# Initialize Groq (The Brain)\n",
        "try:\n",
        "    groq_client = Groq(api_key=st.secrets[\"GROQ_API_KEY\"])\n",
        "except:\n",
        "    st.error(\"üîë Groq API Key missing! Set it in Streamlit Secrets.\")\n",
        "\n",
        "# Initialize Pinecone (The Memory)\n",
        "try:\n",
        "    pc = Pinecone(api_key=st.secrets[\"PINECONE_API_KEY\"])\n",
        "    index_name = \"rag-index\"\n",
        "\n",
        "    # Check if index exists, connect to it\n",
        "    if index_name not in pc.list_indexes().names():\n",
        "        st.warning(f\"Index '{index_name}' not found. Please create it in Pinecone Console (Dim: 384).\")\n",
        "    index = pc.Index(index_name)\n",
        "except:\n",
        "    st.error(\"üîë Pinecone API Key missing! Set it in Streamlit Secrets.\")\n",
        "\n",
        "# Initialize Embeddings (The Translator)\n",
        "# We use FastEmbed because it runs on CPU (Free) and is faster than downloading big models\n",
        "@st.cache_resource\n",
        "def get_embedding_model():\n",
        "    return TextEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "embed_model = get_embedding_model()\n",
        "\n",
        "# --- 3. SIDEBAR: DATA INGESTION ---\n",
        "with st.sidebar:\n",
        "    st.header(\"Upload Knowledge\")\n",
        "    uploaded_file = st.file_uploader(\"Upload a .txt file\", type=\"txt\")\n",
        "\n",
        "    if uploaded_file and st.button(\"Ingest Data\"):\n",
        "        with st.spinner(\"Embedding & Indexing...\"):\n",
        "            # 1. Read Text\n",
        "            text = uploaded_file.read().decode(\"utf-8\")\n",
        "            chunks = [text[i:i+500] for i in range(0, len(text), 500)] # Simple chunking\n",
        "\n",
        "            # 2. Embed Text (Turn into numbers)\n",
        "            embeddings = list(embed_model.embed(chunks))\n",
        "\n",
        "            # 3. Upload to Pinecone\n",
        "            vectors = []\n",
        "            for i, (chunk, emb) in enumerate(zip(chunks, embeddings)):\n",
        "                vectors.append({\n",
        "                    \"id\": f\"chunk_{i}_{int(time.time())}\",\n",
        "                    \"values\": emb.tolist(),\n",
        "                    \"metadata\": {\"text\": chunk}\n",
        "                })\n",
        "\n",
        "            index.upsert(vectors=vectors)\n",
        "            st.success(f\"‚úÖ Indexed {len(chunks)} chunks!\")\n",
        "\n",
        "# --- 4. MAIN CHAT INTERFACE ---\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display history\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# Handle User Input\n",
        "if prompt := st.chat_input(\"Ask about your uploaded data...\"):\n",
        "    # 1. Show User Message\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "\n",
        "    # 2. RETRIEVAL (The \"R\" in RAG)\n",
        "    with st.spinner(\"Thinking...\"):\n",
        "        # Embed the query\n",
        "        query_embedding = list(embed_model.embed([prompt]))[0].tolist()\n",
        "\n",
        "        # Search Pinecone\n",
        "        search_results = index.query(\n",
        "            vector=query_embedding,\n",
        "            top_k=3,\n",
        "            include_metadata=True\n",
        "        )\n",
        "\n",
        "        # Combine Context\n",
        "        context_text = \"\\n\\n\".join([match['metadata']['text'] for match in search_results['matches']])\n",
        "\n",
        "        # 3. GENERATION (The \"G\" in RAG) using Qwen 2.5\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Use the Context below to answer the user.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Context: {context_text}\\n\\nQuestion: {prompt}\"}\n",
        "        ]\n",
        "\n",
        "        chat_completion = groq_client.chat.completions.create(\n",
        "            messages=messages,\n",
        "            model=\"qwen-2.5-32b\", # <--- Using your requested model\n",
        "            temperature=0.5,\n",
        "            max_tokens=1024,\n",
        "        )\n",
        "\n",
        "        response = chat_completion.choices[0].message.content\n",
        "\n",
        "    # 4. Show AI Response\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        st.markdown(response)\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})"
      ]
    }
  ]
}